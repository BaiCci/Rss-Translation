<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新提交：GPT3</title>
    <link>https://www.reddit.com/r/GPT3/new</link>
    <description>AI 文本生成技术的 Reddit 子版块</description>
    <lastBuildDate>Tue, 10 Oct 2023 12:31:43 GMT</lastBuildDate>
    <item>
      <title>GPT 无法解决这个问题 - (xy^2 / x^2y)^2</title>
      <link>https://www.reddit.com/r/GPT3/comments/173ubfp/gpt_is_not_able_to_solve_this_problem_xy2_x2y2/</link>
      <description><![CDATA[怎么解决不了？我以为我能够快速训练代数、线性代数的基础知识，然后进入统计学，这样我就可以学习机器学习的基础知识，然后进入人工智能技术的销售...... I这是 youtube 上的一门课程。 这是我第一次真正迷失方向，需要认真使用 GPT。虽然我以前玩过它。 我要求它重新考虑3次，我向机器提供了正确的答案，但它一直说答案是x2！ 什么我做吗？你知道数学的特殊系统吗？我需要有这个胯部，因为我打算进入营销和销售领域，而不是成为一名真正的机器学习工程师，我只是想了解基础知识，以便潜在的员工会关注我！！！   由   提交 /u/AndrewKorsten   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/173ubfp/gpt_is_not_able_to_solve_this_problem_xy2_x2y2/</guid>
      <pubDate>Mon, 09 Oct 2023 15:14:13 GMT</pubDate>
    </item>
    <item>
      <title>如何限制/截断/剪辑传递给 GPT3.5 的令牌数量</title>
      <link>https://www.reddit.com/r/GPT3/comments/172tjdd/how_to_limittruncateclip_number_of_tokens_being/</link>
      <description><![CDATA[嘿伙计们，我想传递给 GPT3.5 的每一行都有一个 pandas 文本数据框（或数组）。每行中的一些文本非常长，我想限制/截断/剪辑每行中传递给 GPT3.5 的标记数量。 如何限制/截断/剪辑数量每行中有多少个令牌被传递到 GPT3.5？我一直在谷歌搜索并找到这个库 https://github.com/simonw/ttok 但我不确定这是否可行对于我的情况，我需要循环 pandas 数据帧（或数组）中的每一行并限制/截断/剪辑每行中的标记数量。 如果有人可以提供帮助并知道一种方法，我将不胜感激做这个。非常感谢！   由   提交/u/redd-dev  /u/redd-dev  reddit.com/r/GPT3/comments/172tjdd/how_to_limittruncateclip_number_of_tokens_being/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/172tjdd/how_to_limittruncateclip_number_of_tokens_being/</guid>
      <pubDate>Sun, 08 Oct 2023 07:58:59 GMT</pubDate>
    </item>
    <item>
      <title>开放代码解释器</title>
      <link>https://www.reddit.com/r/GPT3/comments/172ndrf/opencodeinterpreter/</link>
      <description><![CDATA[引入 Open-Code-Interpreter，这是将指令转换为代码的终极开源工具。由 HuggingFace 模型提供支持，它可以处理您交给它的任何任务。现在就尝试一下，看看它的神奇之处！ https://github.com/haseeb-heaven/open-code-interpreter 您可以使用任何 Hugging脸部模型，不需要将任何模型下载到您的系统中。 #HuggingFace #chatgpt #gpt #bingai #bardai #python #openai #GPT4   由   提交/u/haseeb-heaven   /u/haseeb-heaven  reddit.com/r/GPT3/comments/172ndrf/opencodeinterpreter/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/172ndrf/opencodeinterpreter/</guid>
      <pubDate>Sun, 08 Oct 2023 02:03:42 GMT</pubDate>
    </item>
    <item>
      <title>【合成数据集生成】“教科书就是你所需要的”……但是如何选择教科书作者呢？ （gpt4、克劳德、bizon、侏罗纪 2...）</title>
      <link>https://www.reddit.com/r/GPT3/comments/171qzf8/synthetic_dataset_generation_textbook_are_all_you/</link>
      <description><![CDATA[我正在寻找比较大型语言模型 (LLM) 性能的资源、论文或经验。我试图找到一个诚实的基准来比较最新大型模型的功能，同时真正对这些模型感兴趣：GPT-3.5 Instruct、GPT-4、Claude 2、Claude Instant 100k、Palm2-Bizon、jurassic-2、LLama2 70 首以及其他最先进的 LLama2 微调。  我对一般基准感兴趣，如果存在的话，对合成数据生成任务的性能比较感兴趣（两者都使用 Phi 和一些 Orca 中使用的“教科书就是你所需要的”方法生成数据/ EvolveInstuct 风格的模型，如 Wizard...）。 编辑：我在标题中的陈述有点挑衅...我对各种合成数据集感兴趣，而不仅仅是“教科书”数据集。概念。   由   提交 /u/BXresearch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/171qzf8/synthetic_dataset_generation_textbook_are_all_you/</guid>
      <pubDate>Fri, 06 Oct 2023 23:20:05 GMT</pubDate>
    </item>
    <item>
      <title>人工智能是 72% 首席执行官的首要投资重点</title>
      <link>https://www.reddit.com/r/GPT3/comments/171kha2/ai_is_the_top_investment_priority_for_72_of_ceos/</link>
      <description><![CDATA[毕马威 (KPMG) 的一项新调查显示，首席执行官对人工智能投资感到兴奋，但对风险的担忧依然存在。 （来源） &lt; p&gt;如果您想先于其他人了解最新的 AI 更新，先查看此处 All In人工智能  72% 的人将生成式人工智能视为首要投资重点。 57% 的人在技术上的支出超过了员工再培训的支出。  62% 的人预计 3-5 年内获得投资回报，表现出长期的前景。  持续的担忧  最关心的问题是实施人工智能面临的道德挑战。 85% 的人认为人工智能是网络安全的一把双刃剑。 81% 的人表示监管差距是一个障碍。 &lt; /ul&gt; 不确定的未来  人工智能被视为变革性的，而不是昙花一现的时尚。 但工人失业和社会影响迫在眉睫 有关生成式人工智能的规则尚未确定。  PS：获取最新的人工智能开发、工具和使用加入发展最快的人工智能时事通讯之一来了解案例。加入 5000 多名专业人士，在 AI 领域变得更加聪明。    ;由   提交/u/Ok-Feeling-1743   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/171kha2/ai_is_the_top_investment_priority_for_72_of_ceos/</guid>
      <pubDate>Fri, 06 Oct 2023 18:52:27 GMT</pubDate>
    </item>
    <item>
      <title>这篇文章是 100% GPT，对吧？</title>
      <link>https://www.reddit.com/r/GPT3/comments/171ehc6/this_article_is_100_gpt_right/</link>
      <description><![CDATA[       由   提交 /u/GoWayLowForThePesos   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/171ehc6/this_article_is_100_gpt_right/</guid>
      <pubDate>Fri, 06 Oct 2023 14:57:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] MIT、Meta、CMU 研究人员：用有限注意力窗口训练的法学硕士可以扩展到无限序列长度，无需任何微调</title>
      <link>https://www.reddit.com/r/GPT3/comments/171ac6m/r_mit_meta_cmu_researchers_llms_trained_with_a/</link>
      <description><![CDATA[ 由   提交 /u/Additional_Zebra_861   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/171ac6m/r_mit_meta_cmu_researchers_llms_trained_with_a/</guid>
      <pubDate>Fri, 06 Oct 2023 11:54:32 GMT</pubDate>
    </item>
    <item>
      <title>微调和标准语言模型在模仿活生生的历史人物方面的比较评估：详细的研究建议</title>
      <link>https://www.reddit.com/r/GPT3/comments/1718aup/comparative_evaluation_of_finetuned_and_standard/</link>
      <description><![CDATA[ 由   提交/u/alcanthro  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/1718aup/comparative_evaluation_of_finetuned_and_standard/</guid>
      <pubDate>Fri, 06 Oct 2023 09:59:14 GMT</pubDate>
    </item>
    <item>
      <title>从AI助手到图像重塑器：Meta的新AI功能</title>
      <link>https://www.reddit.com/r/GPT3/comments/170qyjp/from_ai_assistant_to_image_restyler_metas_new_ai/</link>
      <description><![CDATA[       由   提交 /u/Additional_Zebra_861   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/170qyjp/from_ai_assistant_to_image_restyler_metas_new_ai/</guid>
      <pubDate>Thu, 05 Oct 2023 19:30:13 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 关于训练数据合理使用而非侵权的官方理由</title>
      <link>https://www.reddit.com/r/GPT3/comments/170os6m/openais_official_justification_to_why_training/</link>
      <description><![CDATA[OpenAI 认为当前的合理使用原则可以满足人工智能系统的基本训练需求。但不确定性会带来问题，因此确认这一点的权威裁决将负责任地加速进展。 （完整 PDF） 如果如果您想先于其他人获得最新的 AI 更新，请先查看此处 培训 AI 属于合理使用根据版权法  人工智能培训具有变革性；重新利用可以实现不同的目标。 有效地训练人工智能系统需要完整的副本。 训练数据不公开，避免市场替代。 工作性质和商业用途并不是那么重要的因素。  支持版权框架内的人工智能进步  寻找培训合理使用的原则能够实现持续的人工智能创新。 符合数据计算分析的判例法。 符合合理使用法定因素，特别是变革性目的。  ul&gt; 不确定性阻碍发展  缺乏明确的指导会给人工智能创造者带来成本和法律风险。 权威裁定培训合理使用将消除障碍。 在允许人工智能进步的同时维护版权法。  PS：获取最新的人工智能加入增长最快的人工智能新闻通讯之一，了解开发、工具和用例。加入 5000 多名专业人士，在 AI 领域变得更加聪明。    ;由   提交/u/Ok-Feeling-1743   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/170os6m/openais_official_justification_to_why_training/</guid>
      <pubDate>Thu, 05 Oct 2023 18:04:52 GMT</pubDate>
    </item>
    <item>
      <title>揭开神经网络背后的谜团：它们如何学习？</title>
      <link>https://www.reddit.com/r/GPT3/comments/170nc5v/unraveling_the_mystery_behind_neural_networks_how/</link>
      <description><![CDATA[    /u/Additional_Zebra_861   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/170nc5v/unraveling_the_mystery_behind_neural_networks_how/</guid>
      <pubDate>Thu, 05 Oct 2023 17:06:46 GMT</pubDate>
    </item>
    <item>
      <title>AI 简化：使用 SuperDuperDB 在 Atlas 上进行 MongoDB 矢量搜索</title>
      <link>https://www.reddit.com/r/GPT3/comments/170g2g2/ai_simplified_mongodb_vectorsearch_on_atlas_with/</link>
      <description><![CDATA[嘿大家， 我想分享一些非常有趣的东西。你们听说 MongoDB 现在支持 Atlas 上的矢量搜索了吗？ 这对我们这个领域来说是一个巨大的飞跃，很高兴看到这样的进步。这可以减轻我们在实现人工智能应用程序时遇到的许多困难： https://www.mongodb.com/library/vector-search/building-generative-ai-applications-using-mongodb 同样引起我注意的是与 SuperDuperDB（开源）一起使用它，这似乎是有效利用此功能的游戏规则改变者。令我惊讶的是，有一种方法可以使用 SuperDuperDB 只需一个命令即可利用 MongoDB 的功能。 希望听到您对此的想法。你们有人尝试过这个吗？以下是包含更多信息的博客文章： https://docs .superduperdb.com/blog/2023/09/30/jump-start-ai-development/    由   提交 /u/Sevyten   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/170g2g2/ai_simplified_mongodb_vectorsearch_on_atlas_with/</guid>
      <pubDate>Thu, 05 Oct 2023 11:56:52 GMT</pubDate>
    </item>
    <item>
      <title>一个应用程序，它获取代码，缩小代码，通过人工智能运行它，并将代码更改写回文件系统</title>
      <link>https://www.reddit.com/r/GPT3/comments/170dftd/an_application_that_takes_code_minifies_it_runs/</link>
      <description><![CDATA[       由   提交/u/moonshinemclanmower   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/170dftd/an_application_that_takes_code_minifies_it_runs/</guid>
      <pubDate>Thu, 05 Oct 2023 09:23:26 GMT</pubDate>
    </item>
    <item>
      <title>深入了解 GPT-Synthesizer 和基于 LLM 的代码生成的想法</title>
      <link>https://www.reddit.com/r/GPT3/comments/17024w0/looking_inside_gptsynthesizer_and_the_idea_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/17024w0/looking_inside_gptsynthesizer_and_the_idea_of/</guid>
      <pubDate>Wed, 04 Oct 2023 23:23:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM、向量 DB 和摄取策略的任意组合评估检索增强生成 (RAG)</title>
      <link>https://www.reddit.com/r/GPT3/comments/16zs783/evaluating_retrievalaugmented_generation_rag_with/</link>
      <description><![CDATA[为了帮助开发人员测试他们的 RAG 系统，我们在我们的开源库中添加了一个 RAG 实验类  com/hegelai/prompttools&quot;&gt;PromptTools。它允许用户轻松试验 LLM 和向量 DB 的不同组合，并评估整个流程的结果。 特别是，您可以试验：  分块将文档变成不同的大小 以各种方式预处理这些文档 使用各种矢量化器和嵌入函数将这些文档插入到矢量数据库中，并使用不同的距离函数访问它们&lt; /li&gt;  在我们的 RAG 示例 ，我们从 ChromaDB 检索文档并将它们与我们的提示一起传递到 OpenAI 的聊天模型中。然后，我们将结果传递到内置评估函数（例如语义相似度和自动评估）中，以定量评估您的结果。 PromptTools 与您使用的 LLM 和向量 DB 无关。您可以轻松迭代 RAG 的不同系统架构。您甚至可以引入自己的微调模型或编写自定义集成。此外，您还可以编写自己的评估指标，并独立评估检索步骤。 我们当前的集成包括：  LLM：OpenAI（聊天，微调）、Anthropic、Google Vertex/PaLM、Llama（本地或通过复制） Vector DB：Chroma、Weaviate、 LanceDB、Pinecone、Qdrant 框架：LangChain、MindsDB  通过安装库和 运行此示例。 作为开源维护者，我们始终感兴趣倾听社区的痛点和要求。让我们知道您如何测试 RAG 系统以及我们如何提供帮助。   由   提交/u/hegel-ai   /u/hegel-ai  reddit.com/r/GPT3/comments/16zs783/evaluating_retrievalaugmented_ Generation_rag_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/GPT3/comments/16zs783/evaluating_retrievalaugmented_generation_rag_with/</guid>
      <pubDate>Wed, 04 Oct 2023 16:46:26 GMT</pubDate>
    </item>
    </channel>
</rss>