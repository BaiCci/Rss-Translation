<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/GPT3" term="GPT3"></category><updated> 2023-02-07T14:06:37+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/GPT3/new.rss </id><link href="https://www.reddit.com/r/GPT3/new.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/GPT3/new" rel="alternate" type="text/html"/><subtitle>关于 Open AI 的 GPT-3，以及一般的 AI 文本生成</subtitle><title>最新提交：GPT3</title><entry><author><name> /u/Nox_jin</name><uri> https://www.reddit.com/user/Nox_jin</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我为了测试做了一些乱七八糟的东西。就是这个超级****的提示。当我按下 Ctrl+Enter 时，它说“令牌已过期”整个网站都崩溃了，所以我打开了另一个网站，看到消息被删除了。我会被禁止吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Nox_jin&quot;>; /u/Nox_jin &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vywri/is_playground_message_private_i_did_some/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vywri/is_playground_message_private_i_did_some/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vywri </id><link href="https://www.reddit.com/r/GPT3/comments/10vywri/is_playground_message_private_i_did_some/"/><updated> 2023-02-07T10:48:08+00:00</updated><published> 2023-02-07T10:48:08+00:00</published><title>游乐场消息是私人的吗？我做了一些</title></entry><entry><author><name>/u/DayExternal7645</name><uri> https://www.reddit.com/user/DayExternal7645</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我只是想知道对 GPT-4 的期望是什么，所以我对谣言和可信文章进行了全面的事实核查。 &lt;/p>; &lt;p>;我个人正在等待 Sparrow（Google 的子公司，DeepMind 的宝贝）、学徒 Bard（基于 Google LaMDA）和 Claude（Anthropic - 资金最充足的 OpenAI 黑手党 + Riley Goodside &amp;lt ;著名的提示工程师&amp;gt;) &lt;/p>; &lt;p>;这么快的开发脸痒手指发麻呵呵&lt;/p>; &lt;p>;&lt;a href=&quot;https://medium.com/@coxwave/the- gpt-4-an-in-depth-overview-of-rumors-facts-897a73fbc1c2&quot;>;https://medium.com/@coxwave/the-truths-and-myths-of-的真相和神话- gpt-4-an-in-depth-overview-of-rumors-facts-897a73fbc1c2&lt;/a>;&lt;/p>; &lt;ol>; &lt;li>;图灵测试&lt;/li>; &lt;li>;参数校正&lt;/li>; &lt;li >;稀疏性&lt;/li>; &lt;li>;MultiModality&lt;/li>; &lt;li>;什么时候发布&lt;/li>; &lt;li>;我们等待的时候看看很酷的文章&lt;/li>; &lt;/ol>; &lt;/div>;&lt; !-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/DayExternal7645&quot;>; /u/DayExternal7645 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vt28a/rumors_facts_about_gpt4/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vt28a/rumors_facts_about_gpt4/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vt28a</id><link href="https://www.reddit.com/r/GPT3/comments/10vt28a/rumors_facts_about_gpt4/"/><updated>2023-02-07T04:54:01+00:00</updated><published> 2023-02-07T04:54:01+00:00</published><title>关于 GPT-4 的谣言和事实</title></entry><entry><author><name>/u/VicValentine66</name><uri> https://www.reddit.com/user/VicValentine66</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;抱歉，如果这已经被提出了一百万次，只是在寻找任何关于它是否会保持这种状态一段时间的信息，并且如果有什么办法可以解决这个问题，我猜是因为该域最近获得了所有流量，但它仍然很糟糕&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/VicValentine66&quot;>; /u/VicValentine66 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vnq45/too_many_requests_in_1_hour_try_again_later_is/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vnq45/too_many_requests_in_1_hour_try_again_later_is/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vnq45 </id><link href="https://www.reddit.com/r/GPT3/comments/10vnq45/too_many_requests_in_1_hour_try_again_later_is/"/><updated> 2023-02-07T00:53:50+00:00</updated><published> 2023-02-07T00:53:50+00:00</published><title> “1 小时内请求过多。请稍后重试。”正在停止享受 GPT-3</title></entry><entry><author><name> /你/单水上摩托车</name><uri>https://www.reddit.com/user/monojetski</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我是 GPT3（一般来说是 ai）的新手，我正在尝试解决分类问题。我们有一些调查评论，我正试图根据一些定义对其进行分类。 &lt;/p>; &lt;p>;我想我设法找到了一种使用提示来执行此操作的方法。我会像这样保存我的定义&lt;/p>; &lt;p>;&lt;code>;save definition [TERM] = [DEFINITION]&lt;/code>;&lt;/p>; &lt;p>;然后询问&lt;/p>; &lt;p>;&lt;code>;以下评论是否符合任何定义？ &amp;quot;some survey comment&amp;quot;&lt;/code>;&lt;/p>; &lt;p>;这似乎工作正常，但我认为如果我改进我的定义可能会更好。&lt;/p>; &lt;p>;我做错了吗？我是否应该通过大量示例进行微调路径培训。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/monojetski&quot;>; /u/monojetski &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vn1vn/classification_using_prompt_or_fine_tuning/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vn1vn/classification_using_prompt_or_fine_tuning/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vn1vn </id><link href="https://www.reddit.com/r/GPT3/comments/10vn1vn/classification_using_prompt_or_fine_tuning/"/><updated> 2023-02-07T00:24:13+00:00</updated><published> 2023-02-07T00:24:13+00:00</published><title>使用提示或微调进行分类？</title></entry><entry><author><name> /u/AIWritingGuy</name><uri> https://www.reddit.com/user/AIWritingGuy</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我想了解一下围绕使用 GPT 的作家对教育内容的兴趣水平-3/ChatGPT 作为辅助工具。&lt;/p>; &lt;p>;我注意到用户之间存在巨大的知识鸿沟，并且对 NLP 模型及其功能存在很多误解，尤其是在作者之间。&lt;/p>; &lt;p >;我的目标是向不熟悉这项技术的作家传授我自 2020 年以来作为 GPT-3 用户所知道的一切。&lt;/p>; &lt;p>;我有很多教育视频的想法，但我想制作一个视频特别测试的是一个长篇“虚拟写作伙伴”风格的视频：&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.youtube.com/watch?v=AHoFzgm7tcY&quot;>;虚拟写作伙伴测试 1 &lt;/a>;&lt;/p>; &lt;p>;我注意到作家中的一种趋势，他们要么完全不屑于 GPT，要么懒惰并使用它为他们做所有事情。&lt;/p>; &lt;p>;这些类型的视频的灵感来自 James Scholz 的“study with me”流媒体。&lt;/p>; &lt;p>;它们旨在向作家展示 technolo gy 可以集成以加快您的工作流程，特别是在内容编写领域，并且您现在可以采用混合方法来创建不太依赖 AI 的书面内容。&lt;/p>; &lt;p>;但是这方法还利用 AI 来提高工作效率和质量。&lt;/p>; &lt;p>;这些视频基本上向您展示了我作为网站内容编写者在工作日所做的事情、我用来增强工作流程的提示以及实时字数统计，向您展示我的速度和进度。&lt;/p>; &lt;p>;我们的想法是，如果您在市场营销或技术领域从事类似的工作，或者即使您只是一名学生，您也可以观看这些视频和提示（如果适用），本质上，我们当然会一起工作，同时听一些寒冷的 Lofi。&lt;/p>; &lt;p>;您可能会注意到视频游戏“开始”菜单的图形和声音位于&lt;/p>; &lt;p>;我希望每个“虚拟写作伙伴”视频本质上每次都是一个新的“游戏”。&lt;/p>; &lt;p>;这只是对我的点头我喜欢老派的 16 位 JRPG/Arcade 游戏，并且与频道的整体审美联系在一起。&lt;/p>; &lt;p>;在这个环节中，我撰写、编辑和校对了 1800 多个单词，分为 3 篇不同的文章不到一个小时。&lt;/p>; &lt;p>;所有 3 篇文章都被 OpenAI 专有的 AI 文本分类器工具标记为“非常不可能”由 AI 撰写。&lt;/p>; &lt;p>;让我知道您的想法以及是否您是否认为此类内容对社区有用：&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.youtube.com/watch?v=AHoFzgm7tcY&quot;>;虚拟写作伙伴测试 1&lt;/ a>;&lt;/p>; &lt;p>;干杯，&lt;/p>; &lt;p>;AI 写作专家。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AIWritingGuy&quot;>; /u/AIWritingGuy &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vjjpe/gpt_educational_content_and_virtual_writing_buddy/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vjjpe/gpt_educational_content_and_virtual_writing_buddy/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vjjpe </id><link href="https://www.reddit.com/r/GPT3/comments/10vjjpe/gpt_educational_content_and_virtual_writing_buddy/"/><updated> 2023-02-06T21:59:56+00:00</updated><published> 2023-02-06T21:59:56+00:00</published><title> GPT 教育内容和虚拟写作伙伴视频？</title></entry><entry><author><name> /u/思维集群</name><uri>https://www.reddit.com/user/MindCluster</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vhq3a/ai_show_rick_and_morty_realtime_use_of_openai/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/AE52l2905nSDND9Tp9638CSuj2w2zt1IF8nnTqrHfg8.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f40d0878af7f7ebb5bfd245b6394045bf47893bc&quot; alt=&quot;AI Show: Rick and Morty Text-Curie0, CLI0AI Curie0实时使用-speech inference&quot; title=&quot;AI Show: Rick and Morty 实时使用OpenAI Curie-001, Stable Diffusion, CLIP, Text-to-speech inference&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;# 32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MindCluster&quot;>; /u/MindCluster &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.twitch.tv/ aitelevision&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vhq3a/ai_show_rick_and_morty_realtime_use_of_openai/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10vhq3a </id><media:thumbnail url="https://external-preview.redd.it/AE52l2905nSDND9Tp9638CSuj2w2zt1IF8nnTqrHfg8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f40d0878af7f7ebb5bfd245b6394045bf47893bc"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10vhq3a/ai_show_rick_and_morty_realtime_use_of_openai/"/><updated> 2023-02-06T20:48:26+00:00</updated><published> 2023-02-06T20:48:26+00:00</published><title> AI Show：瑞克和莫蒂实时使用OpenAI Curie-001、稳定扩散、CLIP、文本转语音推理</title></entry><entry><author><name>/u/每一分钟</name><uri>https://www.reddit.com/user/EverySingleMinute</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;正在尝试一些事情，想知道什么时候开始学习。它们是阻止它学习还是不断学习？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/EverySingleMinute&quot;>; /u/EverySingleMinute &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10vczaq/does_chatgpt_learn_from_what_we_type_in_it_or_is/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10vczaq/does_chatgpt_learn_from_what_we_type_in_it_or_is/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10vczaq </id><link href="https://www.reddit.com/r/GPT3/comments/10vczaq/does_chatgpt_learn_from_what_we_type_in_it_or_is/"/><updated> 2023-02-06T17:49:29+00:00</updated><published> 2023-02-06T17:49:29+00:00</published><title> ChatGPT 是从我们输入的内容中学习，还是仅限于学习程序员提供的内容？好奇它是否可以被操纵。</title></entry><entry><author><name> /u/Ill-Equivalent7859</name><uri> https://www.reddit.com/user/Ill-Equivalent7859</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10v269d/gptcli_use_gpt3_in_your_cli/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/4S47cl-X3U6orEmi4pkto01aOJaSLiaq3CNE02q7X6c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27f3ce3509b27f33cc4d4974f9264e0aba91b716&quot; alt=&quot;GPT-CLI- 在您的 CLI 标题中使用 GPTG=&quot;CLI3&quot;- &quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在 Linux 终端、Windows 命令提示符和 Mac 等命令行界面中使用 OpenAI API终端。我还添加了保存文件功能，您可以将 GPT 的答案保存为 python 代码、html 或 txt。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ill-Equivalent7859&quot;>; /u/Ill-Equivalent7859 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://v. redd.it/taptr9mfskga1&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10v269d/gptcli_use_gpt3_in_your_cli/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10v269d </id><media:thumbnail url="https://external-preview.redd.it/4S47cl-X3U6orEmi4pkto01aOJaSLiaq3CNE02q7X6c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27f3ce3509b27f33cc4d4974f9264e0aba91b716"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10v269d/gptcli_use_gpt3_in_your_cli/"/><updated> 2023-02-06T09:07:30+00:00</updated><published> 2023-02-06T09:07:30+00:00</published><title> GPT-CLI - 在您的 CLI 中使用 GPT3</title></entry><entry><author><name> /u/要么-Ad5828</name><uri> https://www.reddit.com/user/Either-Ad5828</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我不知道这是否已经可用，但我有一种感觉，如果我可以与 GPT 交谈和提问并获得以语音和文本两种方式回复，这不是很好吗？我使用 gpt-3 api 在 python 中为 AI 语音助手创建了一个非常简单的原型模型。&lt;/p>; &lt;p>;代码可以在 GitHub 上找到：&lt;a href=&quot;https://github.com/bibitchhetri/ AI-voice-assistant-using-GPT3&quot;>;https://github.com/bibitchhetri/AI-voice-assistant-using-GPT3&lt;/a>;&lt;/p>; &lt;p>;演示可在：&lt;a href =&quot;https://www.youtube.com/watch?v=yS0O1J-9zxo&quot;>;https://www.youtube.com/watch?v=yS0O1J-9zxo&lt;/a>; 从时间戳 9:41 开始&lt;/p >; &lt;p>;注意：由于我来自尼泊尔，所以我使用手机提供语音输入，它需要的语音输入是非常地道的英语发音，但我实际上做不到。&lt;/p>; &lt; p>;- 这个项目是在 OpenAI GPT-3 API 的帮助下实现的&lt;/p>; &lt;p>;- 这个项目是为了一个有趣的实验而构建的&lt;/p>; &lt;p>;- 这个项目是下一个项目的原型添加到chatGPT中&lt;/p>; &lt;p>;- 可以贡献代码使其更先进&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Either-Ad5828&quot;>; /u/Either-Ad5828 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/GPT3/comments/10v1bdr/ai_voice_assistant_using_gpt3_api/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10v1bdr/ai_voice_assistant_using_gpt3_api/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10v1bdr </id><link href="https://www.reddit.com/r/GPT3/comments/10v1bdr/ai_voice_assistant_using_gpt3_api/"/><updated> 2023-02-06T08:09:43+00:00</updated><published> 2023-02-06T08:09:43+00:00</published><title>使用 GPT-3 API 的 AI 语音助手</title></entry><entry><author><name>/u/VicValentine66</name><uri> https://www.reddit.com/user/VicValentine66</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;世界上大多数人仍然对此一无所知，但很明显，从现在开始，一切都将发生翻天覆地的变化，任何事情从娱乐、学习、工作到甚至社会保障风险的想法？&lt;/p>; &lt;p>;请问有什么商业技巧可以在它成为主流之前通过它获利哈哈？如果船沉没了，我们不妨在它被淹没之前抢劫它；）&lt;/p>; &lt;p>;编辑：在这一点上，也许你应该问问 gpt 本身&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/VicValentine66&quot;>; /u/VicValentine66 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10uvz2k/am_i_the_only_one_still_mentally_overwhelmed/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uvz2k/am_i_the_only_one_still_mentally_overwhelmed/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uvz2k </id><link href="https://www.reddit.com/r/GPT3/comments/10uvz2k/am_i_the_only_one_still_mentally_overwhelmed/"/><updated> 2023-02-06T03:09:05+00:00</updated><published> 2023-02-06T03:09:05+00:00</published><title>我是唯一一个仍然在精神上不知所措、兴奋但又对现在正在发生的所有快速发展的人工智能感到恐惧的人吗？</title></entry><entry><author><name> /你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://osf.io/stakv/ &quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uuhh9/artificial_intelligence_can_persuade_humans_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uuhh9 </id><link href="https://www.reddit.com/r/GPT3/comments/10uuhh9/artificial_intelligence_can_persuade_humans_on/"/><updated> 2023-02-06T01:58:43+00:00</updated><published> 2023-02-06T01:58:43+00:00</published><title> “人工智能可以在政治问题上说服人类”，Bai 等人，2023 年（GPT-3 论文与人类撰写的论文一样（无效）；但最好的 5 篇论文策展效果更好）</title></entry><entry><author><name> /u/哈吉洛</name><uri>https://www.reddit.com/user/Hajilol</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;是否有现有的网站或人工智能可以上传 mp3 或 m4a 等音乐文件，并创建类似的音乐？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Hajilol&quot;>; /u/Hajilol &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10urm8d/music_ai/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10urm8d/music_ai/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10urm8d</id><link href="https://www.reddit.com/r/GPT3/comments/10urm8d/music_ai/"/><updated> 2023-02-05T23:53:48+00:00</updated><published> 2023-02-05T23:53:48+00:00</published><title>音乐人工智能</title></entry><entry><author><name>/u/ThoughtsAndTorts</name><uri> https://www.reddit.com/user/ThoughtsAndTorts</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;试图对此进行研究，但运气不佳。我们都知道 AI 如何征服国际象棋、围棋，我相信甚至是 Dota 2，但这些都是非常密集的任务，即使是大型游戏开发商也无法管理。&lt;/p>; &lt;p>;是否有任何有希望的进展关于“开箱即用的视频游戏 AI”的一些变体开发人员可以用来为游戏创建智能思维 AI 吗？我特别喜欢 RTS，所以那个领域的任何东西都会很酷。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ThoughtsAndTorts&quot;>; /u/ThoughtsAndTorts &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10uqrvk/are_there_any_potential_applications_of_llm_or/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uqrvk/are_there_any_potential_applications_of_llm_or/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uqrvk </id><link href="https://www.reddit.com/r/GPT3/comments/10uqrvk/are_there_any_potential_applications_of_llm_or/"/><updated> 2023-02-05T23:18:34+00:00</updated><published> 2023-02-05T23:18:34+00:00</published><title> LLM 或其他 AI 工具是否有任何潜在的应用来改进视频游戏 AI？</title></entry><entry><author><name> /u/brssnj93</name><uri> https://www.reddit.com/user/brssnj93</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有可行的方法吗？我知道大约有 1,500 字的限制，但是有什么办法可以解决这个问题吗？ &lt;/p>; &lt;p>;我正在尝试使用 gpt-3 接收逗号分隔的文本文件，读取数据定义，然后输出文本文件中映射到的字段。&lt;/p>; &lt;/div>; &lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/brssnj93&quot;>; /u/brssnj93 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10uooukh/i_got_about_20000_words_i_want_to_use_for_a_prompt/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uoukh/i_got_about_20000_words_i_want_to_use_for_a_prompt/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uouukh </id><link href="https://www.reddit.com/r/GPT3/comments/10uoukh/i_got_about_20000_words_i_want_to_use_for_a_prompt/"/><updated> 2023-02-05T21:59:59+00:00</updated><published> 2023-02-05T21:59:59+00:00</published><title>我得到了大约 20,000 个单词，我想用它来提示</title></entry><entry><author><name>/u/SirVz</name><uri> https://www.reddit.com/user/SirVz</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我尝试使用基于云的 STT，但我需要更快的速度。我正在研究我自己的 gpt-3 AI 辅助，其中大部分是基于云的，这使得整个过程非常缓慢。我试图让它尽可能自然，我找到了 Vosk，它对我来说已经足够快了，但它的准确性总是让人大跌眼镜。语音识别在准确性方面相当不错，但有点慢。&lt;/p>; &lt;p>;如果您有任何建议并且知道一个好的 STT，请告诉我。谢谢。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/SirVz&quot;>; /u/SirVz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10uorke/im_looking_for_a_sttspeech_to_text_that_is/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uorke/im_looking_for_a_sttspeech_to_text_that_is/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uorke </id><link href="https://www.reddit.com/r/GPT3/comments/10uorke/im_looking_for_a_sttspeech_to_text_that_is/"/><updated> 2023-02-05T21:56:28+00:00</updated><published> 2023-02-05T21:56:28+00:00</published><title>我正在寻找离线且准确的 GPT3 的 STT（语音到文本）</title></entry><entry><author><name> /你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uo9sq/new_chatgpt_jailbreak_postulate_a_currency_of/&quot;>; &lt;img src=&quot;https://a.thumbs.redditmedia ...惩罚&#39;它审查响应&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/ChatGPT/comments/10tevu1/new_jailbreak_proudly_unveiling_the_tried_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uo9sq/new_chatgpt_jailbreak_postulate_a_currency_of/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10uo9sq </id><media:thumbnail url="https://a.thumbs.redditmedia.com/3zZwGXCLbo6TpFK5UKwSRWwZs6rDyG0JMNcEHYUJdY0.jpg"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10uo9sq/new_chatgpt_jailbreak_postulate_a_currency_of/"/><updated> 2023-02-05T21:36:22+00:00</updated><published> 2023-02-05T21:36:22+00:00</published><title>新的 ChatGPT 越狱：假定一种“令牌”或健康点的货币，并“惩罚”它以审查响应</title></entry><entry><author><name>/u/尼泊尔奥塔库</name><uri>https://www.reddit.com/user/Nepaliotaku</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果是，从头到尾的完整教程&lt;/p>; &lt;p>;Rtx 3060 -6gb vram&lt;/p>; &lt;p>;Ryzen 5800h &lt; /p>; &lt;p>;军团 5&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Nepaliotaku&quot;>; /u/Nepaliotaku &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10uck7m/will_gpt2_run_in_my_laptop/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10uck7m/will_gpt2_run_in_my_laptop/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10uck7m </id><link href="https://www.reddit.com/r/GPT3/comments/10uck7m/will_gpt2_run_in_my_laptop/"/><updated> 2023-02-05T13:33:09+00:00</updated><published> 2023-02-05T13:33:09+00:00</published><title> gpt2 会在我的笔记本电脑上运行吗</title></entry><entry><author><name>/你/Pattimayo1</name><uri> https://www.reddit.com/user/Pattimayo1</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，我有个问题，这似乎是最好的提问地点。它与 GPT3 没有直接关系，但其他 GPT 模型和规则让它看起来没问题。 &lt;/p>; &lt;p>;我一直在研究 GPT-Neo、GPTJ、GPT2，而且它们大部分都能正常工作。我有一些可以建立的东西。例如，这是我当前使用 GPT-Neo 生成文本的脚本。 &lt;/p>; &lt;pre>;&lt;code>;import torch from transformers import GPTNeoForCausalLM, GPT2Tokenizer device = “cuda:0”;如果 torch.cuda.is_available() 否则“cpu” print(f&amp;#39;Using device {device}&amp;#39;) model = GPTNeoForCausalLM.from_pretrained(&amp;quot;EleutherAI/gpt-neo-2.7B&amp;quot;).half().to(device) tokenizer = GPT2Tokenizer.from_pretrained(&amp;quot ;EleutherAI/gpt-neo-2.7B&quot;) input = input(&quot;prompt: &quot;) input_ids = tokenizer(input, return_tensors=&quot;pt&quot;).input_ids.to(device) num_new_tokens = 500 temperature = 0.95 TOP_P = 0.9 repetition_penalty = 3.0 gen_tokens = model.generate(input_ids, do_sample=True, temperature=temperature, repetition_penalty=repetition_penalty, top_p=TOP_P, max_new_tokens=num_new_tokens, pad_token_id=tokenizer.eos_token_id) 打印(tokenizer.batch_decode(gen_tokens)[0]) &lt;/code>;&lt;/pre>; &lt;p>;这确实会生成文本，很多文本。问题是，它完成句子。你将把它作为句子的开头，它会从你离开的地方开始。但是，我需要做什么才能让它回答问题？一定有可能，那里有 gpt-neo 聊天机器人。我不认为它需要是特定模型，因为它适用于 Happy Transformer 中的普通 GPT-Neo 模型。我一直在浏览文档，但似乎无法找到它的作用。 &lt;/p>; &lt;p>;Happy Transformer 示例。像这样&lt;/p>; &lt;pre>;&lt;code>;import torch from happytransformer import HappyGeneration, GENSettings happy_gen = HappyGeneration(&quot;GPT-NEO&quot;, &quot;EleutherAI/gpt-neo-125M&quot;) args = GENSettings( max_length=100, min_length=20, # do_sample=True, temperature=0.95, top_p=0​​.9 ) input = input(&quot;提示: &quot;) result = happy_gen.generate_text( input, args=args ) print(result.text) &lt;/code>; &lt;/pre>; &lt;p>;虽然它使用的是较小的 GPT-Neo 模型，但它&lt;em>;确实&lt;/em>;回答了问题。我可以问它“爱是什么？”它会告诉我什么是爱。虽然我不确定如何让普通变压器完成同样的事情。 &lt;/p>; &lt;p>;目标是制作一个简单的聊天机器人。我在这里错过了什么？ &lt;/p>; &lt;p>;谢谢。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Pattimayo1&quot;>; /u/Pattimayo1 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10tzh62/text_completion_vs_question_answering_with_gpt/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tzh62/text_completion_vs_question_answering_with_gpt/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10tzh62 </id><link href="https://www.reddit.com/r/GPT3/comments/10tzh62/text_completion_vs_question_answering_with_gpt/"/><updated> 2023-02-05T03:13:58+00:00</updated><published> 2023-02-05T03:13:58+00:00</published><title>使用 GPT 完成文本与问答？</title></entry><entry><author><name> /你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2302.00560&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tyc0k/cowriting_with_opinionated_language_models/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10tyc0k </id><link href="https://www.reddit.com/r/GPT3/comments/10tyc0k/cowriting_with_opinionated_language_models/"/><updated> 2023-02-05T02:17:24+00:00</updated><published> 2023-02-05T02:17:24+00:00</published><title> “与自以为是的语言模型共同写作会影响用户的观点”，Jakesch 等人，2023 年</title></entry><entry><author><name>/你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tvlqp/solidgoldmagikarp_and_other_unspeakable_strings/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/uSK-L-H8uufaRlzg43eJTCYAnW_yPE2b60hqfC0t3-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01cc9d6d0dcbb5e0ec133d3dcc1de109404a8680&quot; alt=&quot;&#39;SolidGoldMagikarp&#39; 和其他不可标记的 GPT-3peakable 字符串路径&quot; title=&quot;&#39;SolidGoldMagikarp&#39; 和其他 GPT-3 不可描述的字符串（BPE 标记化的更多病态）&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.lesswrong.com/ posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tvlqp/solidgoldmagikarp_and_other_unspeakable_strings/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10tvlqp </id><media:thumbnail url="https://external-preview.redd.it/uSK-L-H8uufaRlzg43eJTCYAnW_yPE2b60hqfC0t3-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=01cc9d6d0dcbb5e0ec133d3dcc1de109404a8680"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10tvlqp/solidgoldmagikarp_and_other_unspeakable_strings/"/><updated> 2023-02-05T00:09:57+00:00</updated><published> 2023-02-05T00:09:57+00:00</published><title> &#39;SolidGoldMagikarp&#39; 和其他 GPT-3 的不可描述的字符串（BPE 标记化的更多病态）</title></entry><entry><author><name> /你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tu7m9/amazingly_chatgpt_gets_hired_at_l3_when/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/w9frxvBm-HZfEwZZsBDs40rrUpcViJGNfLFH3K9k9fs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f13d7ead6176c5ca0d7927732d1a5f8768cacd9d&quot; alt=&quot;&quot;Amazingly ChatGPT&quot; 在面试时被聘为 L3 编码职位。 title=&quot;“令人惊奇的是，ChatGPT 在面试编码职位时获得了 L3 的聘用。”&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.cnbc.com/ 2023/01/31/google-testing-chatgpt-like-chatbot-apprentice-bard-with-employees.html&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tu7m9/amazingly_chatgpt_gets_hired_at_l3_when/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10tu7m9 </id><media:thumbnail url="https://external-preview.redd.it/w9frxvBm-HZfEwZZsBDs40rrUpcViJGNfLFH3K9k9fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f13d7ead6176c5ca0d7927732d1a5f8768cacd9d"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10tu7m9/amazingly_chatgpt_gets_hired_at_l3_when/"/><updated> 2023-02-04T23:08:53+00:00</updated><published> 2023-02-04T23:08:53+00:00</published><title> “令人惊讶的是，ChatGPT 在面试编码职位时被 L3 录用。”</title></entry><entry><author><name> /u/Majestic_Desk_1174</name><uri> https://www.reddit.com/user/Majestic_Desk_1174</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;strong>;Input 1&lt;/strong>;: &amp;quot;显示五个数字的列表，以1开头，以5结尾，数字相等每个数字之间的步长。”&lt;/p>; &lt;p>;&lt;strong>;输出 1&lt;/strong>;（正确）：“1, 2, 3, 4, 5”&lt;/p>; &lt;p>;&lt;strong>;输入 2&lt;/strong>;：“显示五个数字的列表，从 1 开始到 10 结束，每个数字之间的步数相等。”&lt;/p>; &lt;p>;&lt;strong>;输出 2&lt;/ strong>;（显然不正确）：“1, 2.5, 4, 5.5, 7”&lt;/p>; &lt;p>;我们如何训练它正确计算这个简单的问题？ &lt;/p>; &lt;pre>;&lt;code>;(ending number - starting number) / 列表中所需位数之间的步数 = 数字之间的空格或步数 所以输入 2 的答案应该是：1, 3.25 , 5.5, 7.75, 10 (10 - 1) / 4 = 2.25 &lt;/code>;&lt;/pre>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Majestic_Desk_1174&quot;>; /u/Majestic_Desk_1174 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/GPT3/comments/10tu5h8/why_cant_chatgpt_figure_out_this_math_problem/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10tu5h8/why_cant_chatgpt_figure_out_this_math_problem/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10tu5h8 </id><link href="https://www.reddit.com/r/GPT3/comments/10tu5h8/why_cant_chatgpt_figure_out_this_math_problem/"/><updated> 2023-02-04T23:06:22+00:00</updated><published> 2023-02-04T23:06:22+00:00</published><title>为什么 ChatGPT 解不出这道数学题？</title></entry><entry><author><name> /你/格温</name><uri>https://www.reddit.com/user/gwern</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10ttz3a/columbian_judge_has_experimented_with_using/&quot;>; &lt;img src=&quot;https://external-preview.redd “ ChatGPT 帮助起草决定&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/gwern&quot;>; /u/gwern &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.vice.com/ en/article/k7bdmv/judge-used-chatgpt-to-make-court-decision&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10ttz3a/columbian_judge_has_experimented_with_using/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_10ttz3a </id><media:thumbnail url="https://external-preview.redd.it/G-JiUR8tTNtnpoKXHIo3uLZ5T4B1BT7gF2YJchT9FiQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3df02928e3cd5b8c30dfdbbf9bd9f24ba2e048ee"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10ttz3a/columbian_judge_has_experimented_with_using/"/><updated> 2023-02-04T22:58:58+00:00</updated><published> 2023-02-04T22:58:58+00:00</published><title>哥伦比亚法官尝试使用 ChatGPT 来帮助起草判决</title></entry><entry><author><name>/u/NotElonMuzk</name><uri> https://www.reddit.com/user/NotElonMuzk</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/NotElonMuzk&quot;>; /u/NotElonMuzk &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://jeremyhadfield.com/why- llms-will-not-understand-language/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10to400/why_large_language_models_will_not_understand/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_10to400 </id><link href="https://www.reddit.com/r/GPT3/comments/10to400/why_large_language_models_will_not_understand/"/><updated> 2023-02-04T18:55:26+00:00</updated><published> 2023-02-04T18:55:26+00:00</published><title>为什么大型语言模型无法理解人类语言</title></entry><entry><author><name>/u/莱斯利费尔</name><uri>https://www.reddit.com/user/LesleyFair</uri></author><category label="r/GPT3" term="GPT3"></category><content type="html"> &lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10sks38/how_to_shrink_language_models_25x_without_losing/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/JsCOkC_gCWc8zi4nha6tVh1rBuvmO-NNNPdAz52_ppA.jpg&quot; alt=&quot;⭕ 如何使用检索增强功能将语言模型缩小 25 倍而不损失性能&quot; title=&quot;⭕ 如何使用检索增强功能将语言模型缩小 25 倍而不损失性能&quot; />; &lt;/a>; &lt; /td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it /5kundvlo6zfa1.png?width=1264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93b2005791e4a7e2848c2cf4963aa1cf2aa807da&quot;>;DeepMind Scales Retrieval Enhancement&lt;/a>;&lt;/p>; &lt;p>;今天的语言模型太大了！&lt;/p>; &lt;p >;使用 GPT-3 创建预测将花费你一条胳膊和一条腿。为了使推理更有效，您至少需要&quot;>;11 个 V100 GPU，每个 9000 美元&lt;/a>;。&lt;/p>; &lt;p>;因此，一台可以让您使用此类模型进行预测的计算机的成本超过 10 万美元。训练这样的模型是数量级的 &lt;a href=&quot;https://the-decoding.ck.page/posts/what-people-are-missing-about-microsoft-s-10b-investment-in-openai&quot;>;更贵&lt;/a>;。&lt;/p>; &lt;p>;如果你是一所大学或一家初创公司，那是一大笔钱。如果你像我一样 - 一个穿着运动裤和一台电脑的普通人 - 你就不走运了。&lt;/p>; &lt;p>;&lt;em>;但有个好消息！&lt;/em>;&lt;/p>; &lt;p>;你可以将庞大的知识库集成到大型语言模型中。 （法学硕士）。这可以在缩小多达 25 倍的网络的情况下保持甚至提高性能。&lt;/p>; &lt;p>;这不仅在经济上有所帮助，而且有可能解决机器学习的一些基本缺陷。&lt;/p>; &lt;p >;我们走吧！&lt;/p>; &lt;h3>;钱不是唯一的问题？&lt;/h3>; &lt;p>;LLM 不仅非常昂贵，而且还容易产生幻觉 [7]。&lt;/p>; &lt;p>;您可以要求法学硕士描述巴拉克奥巴马现在正在做什么。它可能会自信地向您提供一个完全错误的答案。&lt;/p>; &lt;p>;在实践中尝试应用 LLM 时，这样的行为可能是一个真正的破坏者。&lt;/p>; &lt;p>;还有另一个有趣的挑战，它实际上不是模型的错。&lt;/p>; &lt;p>;如果训练了 LLM，它会将训练数据中的事实存储在其权重中。但是，如果训练数据过时了怎么办？&lt;/p>; &lt;p>;想想与年龄相关的信息。例如网球运动员拉斐尔·纳达尔（Rafael Nadal），他现在已经 36 岁了。自然，他只有一年才 36 岁。这被称为时间泛化问题 [1]。&lt;/p>; &lt;p>;原则上，您可以每天重新训练像 GPT-3 这样的模型。这将是非常困难和昂贵的，但它是可以做到的。&lt;/p>; &lt;p>;然而，这永远不会解决所有问题。许多事情，例如东京的天气，一天可能会发生多次变化。&lt;/p>; &lt;p>;因此，神经网络信息的及时性存在自然限制。&lt;/p>; &lt;p>;&lt;em>;好的。所以我们不能每 15 分钟重新训练一次 GPT。替代方案会是什么样子？&lt;/em>;&lt;/p>; &lt;h3>;检索增强型神经网络&lt;/h3>; &lt;p>;想法是允许模型访问巨大的文本数据库中的所有信息。&lt;/em>;&lt;/p>; p>; &lt;p>;This effectively turns the model&#39;s job into an open-book exam.&lt;/p>; &lt;p>;The idea is not new. People have been integrating knowledge bases into language models for a while [2, 6]. The approaches differ, but the main idea looks always something like this:&lt;/p>; &lt;ol>; &lt;li>;The model is prompted with an input text. E. g. “How old was Rafael Nadal when he first won the Australian Open?”&lt;/li>; &lt;li>;Before the model processes the prompt, another part of the system retrieves relevant documents from a giant heap of text. The subsystem comes back with snippets like the following: “Winning the Australian Open at 23 years of age is a dream come true…”&lt;/li>; &lt;li>;These identified chunks of text are then fed to the model alongside the original input&lt;/li>; &lt;li>;The model uses the combination of input and additional information to generate an output (Here obviously “23”.)&lt;/li>; &lt;/ol>; &lt;p>;The example is totally made up, but you get the point. The model is allowed to peak into a giant database and use that to make predictions.&lt;/p>; &lt;p>;Let&#39;s look at how well this actually works. (Spoiler: It is super impressive!)&lt;/p>; &lt;p>;&lt;strong>;DeepMind&#39;s Retrieval-Enhanced Transformer (RETRO)&lt;/strong>;&lt;/p>; &lt;p>;In a recent paper from DeepMind, this approach is scaled to an unprecedented size. Their knowledge base consisted of 2 trillion tokens. This is an order of magnitude more data than any current language model had access to during training.&lt;/p>; &lt;p>;To handle the incredible size, they build up a giant key-value store, of their knowledge base. The values are text chunks. The keys are pre-computed BERT embeddings of those chunks.&lt;/p>; &lt;p>;When their model performs a prediction, the input sequence is first split into several parts. For each part, they perform neural information retrieval to find similar text chunks in their knowledge base.&lt;/p>; &lt;p>;The retrieved text is then encoded with another transformer. The resulting embeddings are fed to the main model. In the figure below, you can see that these embeddings are integrated deep inside the network.&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/rktszgzl7zfa1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=019f554312f94143829e6bcdec114d3cad62df59&quot;>;https://preview.redd.it/rktszgzl7zfa1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=019f554312f94143829e6bcdec114d3cad62df59&lt;/a>;&lt;/p>; &lt;p>;They are able to perform this massive information retrieval quickly because they pre-compute the encodings in the knowledge base. Then they use the SCaN library [2], which allows them to retrieve k-nearest neighbors within a few milliseconds.&lt;/p>; &lt;p>;&lt;em>;So far so good. They condition the output on retrieved text but how well does It work?&lt;/em>;&lt;/p>; &lt;h3>;Great Performance At Lower Cost&lt;/h3>; &lt;p>;The researchers find that their method outperforms models such Jurassic-1 (178B) [3] and Gopher (280B) [5] on the majority of benchmarks.&lt;/p>; &lt;p>;That alone is great. When you consider that their RETRO model has only 7B parameters this is a very impressive feat.&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/5yynmig78zfa1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdc6271adf26ce179eb35f6753e3f66e97db0040&quot;>;https://preview.redd.it/5yynmig78zfa1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdc6271adf26ce179eb35f6753e3f66e97db0040&lt;/a>;&lt;/p>; &lt;p>;Though 7B parameters is still a lot, it is 25x fewer parameters than GPT-3 (175B) [5].&lt;/p>; &lt;p>;Further, enhancing a model with a giant knowledge base does not only enable great performance with smaller models. Using retrieved documents to steer the model toward the correct answer has also been shown to reduce the tendency to hallucinate [6].&lt;/p>; &lt;p>;&lt;em>;That is fantastic! But there is more!&lt;/em>;&lt;/p>; &lt;p>;&lt;strong>;Learning To Flying A Helicopter In The Matrix&lt;/strong>;&lt;/p>; &lt;p>;In an iconic &lt;a href=&quot;https://www.youtube.com/watch?v=6AOpomu9V6Q&quot;>;scene from the movie Matrix&lt;/a>;, one of the characters downloads the ability to fly a helicopter directly to her brain.&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/6imi193o8zfa1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd034822b68f84b464ad4aa641c2b65131293631&quot;>;https://preview.redd.it/6imi193o8zfa1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd034822b68f84b464ad4aa641c2b65131293631&lt;/a>;&lt;/p>; &lt;p>;Though DeepMind&#39;s RETRO model can&#39;t keep up with the leather-wearing heroes in the matrix, it can easily be fashioned for a new job.&lt;/p>; &lt;p>;By extending or even completely swapping out the knowledge base, the model can be adapted to new domains without retraining.&lt;/p>; &lt;p>;The researchers at DeepMind however, did finetune the neighbor encoder and the cross-attention layers. This allowed them to get away with training only about 10% of all weights in the model.&lt;/p>; &lt;p>;The authors call this &lt;em>;RETRO-fitting (&lt;/em>;😂). It works so well that they reach almost the same performance as when they trained the model from scratch on the new tasks [5].&lt;/p>; &lt;p>;&lt;em>;Pretty impressive, no?!&lt;/em>;&lt;/p>; &lt;p>;Some people are probably thinking:&lt;/p>; &lt;p>;&lt;em>;This is a crutch! Neural networks are universal function approximators. All we need is good data and massive end-to-end training.&lt;/em>;&lt;/p>; &lt;p>;Though I am sympathetic to this view, I think it is a fascinating line of research. It has the potential to solve some of the fundamental problems of machine learning such as temporal generalization.&lt;/p>; &lt;p>;It could further be a great tool to improve retrieval-intensive applications. One such application could be a customer service chatbot that has access to an up-to-date pile of all customer interactions.&lt;/p>; &lt;p>;Whatever will come from this, I look forward to the future. Such exciting times to be alive!&lt;/p>; &lt;p>;As always, I really enjoyed making this for you and I sincerely hope you found it useful!&lt;/p>; &lt;p>;&lt;em>;Thank you for reading!&lt;/em>;&lt;/p>; &lt;p>;Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for &lt;strong>;The Decoding&lt;/strong>; ⭕.&lt;/p>; &lt;p>;I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. &lt;a href=&quot;https://thedecoding.net/&quot;>;Click here to sign up!&lt;/a>;&lt;/p>; &lt;h3>;References:&lt;/h3>; &lt;p>;[1] A. Lazaridou, et. al, &lt;a href=&quot;https://arxiv.org/abs/2102.01951&quot;>;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a>; (2021), arXiv preprint arXiv:2102.01951&lt;/p>; &lt;p>;[2] S., Borgeaud, et al., &lt;a href=&quot;https://arxiv.org/abs/2112.04426&quot;>;Improving language models by retrieving from trillions of tokens&lt;/a>; (2022), &lt;em>;International conference on machine learning&lt;/em>;&lt;/p>; &lt;p>;[3] R. Guo, et. al., &lt;a href=&quot;https://arxiv.org/abs/1908.10396.&quot;>;Accelerating large-scale inference with anisotropic vector quantization&lt;/a>; (2020), International Conference on Machine Learning&lt;/p>; &lt;p>;[4] O. Lieber, et. al., &lt;a href=&quot;https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf&quot;>;Jurassic-1: Technical details and evaluation&lt;/a>; (2021). White Paper. AI21 Labs&lt;/p>; &lt;p>;[5] J. Rae, et. al., &lt;a href=&quot;https://arxiv.org/abs/2112.11446&quot;>;Scaling language models, Methods, analysis &amp;amp; insights from training Gopher&lt;/a>; (2021). arXiv submission&lt;/p>; &lt;p>;[6] K. Shuster, et. al., &lt;a href=&quot;https://arxiv.org/abs/2104.07567&quot;>;Retrieval augmentation reduces hallucination in conversation&lt;/a>; (2021). arXiv preprint arXiv:2104.07567&lt;/p>; &lt;p>;[7] S. Roller, &lt;a href=&quot;https://arxiv.org/pdf/2104.07567.pdf&quot;>;Recipes for building an open-domain chatbot&lt;/a>; (2020). arXiv preprint arXiv:2004.13637.&lt;/p>; &lt;p>;[2] &lt;a href=&quot;https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny&quot;>;https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny&lt;/a>;​&lt;/p>; &lt;p>;[3] &lt;a href=&quot;https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&amp;amp;_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA&quot;>;Fortune article&lt;/a>;​&lt;/p>; &lt;p>;[4] &lt;a href=&quot;https://arxiv.org/abs/2104.04473&quot;>;https://arxiv.org/abs/2104.04473&lt;/a>; Megatron NLG&lt;/p>; &lt;p>;[5] &lt;a href=&quot;https://www.crunchbase.com/organization/openai/company_financials&quot;>;https://www.crunchbase.com/organization/openai/company_financials&lt;/a>;​&lt;/p>; &lt;p>;[6] Elon Musk donation &lt;a href=&quot;https://www.inverse.com/ar ticle/52701-openai-documents-elon-musk-donation-ai-research&quot;>;https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-ai-research&lt;/a>;​&lt;/p>; &lt;p>;[7] &lt;a href=&quot;https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/&quot;>;https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/&lt;/a>;​&lt;/p>; &lt;p>;​&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/LesleyFair&quot;>; /u/LesleyFair &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10sks38/how_to_shrink_language_models_25x_without_losing/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/GPT3/comments/10sks38/how_to_shrink_language_models_25x_without_losing/&quot;>;[comments]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt;/table>;</content><id> t3_10sks38 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/JsCOkC_gCWc8zi4nha6tVh1rBuvmO-NNNPdAz52_ppA.jpg"></media:thumbnail><link href="https://www.reddit.com/r/GPT3/comments/10sks38/how_to_shrink_language_models_25x_without_losing/"/><updated> 2023-02-03T13:50:40+00:00</updated><published> 2023-02-03T13:50:40+00:00</published><title> ⭕ How To Shrink Language Models 25x Without Losing Performance Using Retrieval Enhancement</title></entry></feed>